% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{A Review of Complex Network Analysis}

Here is a test reference~\cite{Knuth68:art_of_programming}.
These additional lines have been added just to demonstrate the spacing
for the rest of the document. Spacing will differ between the typeset main
document, and typeset individual documents, as the commands
to change spacing for the body of the thesis are only in the main document.

\section{Link Prediction}\label{previous:link-prediction}
Link prediction is the problem of determining future or missing associations between entities in complex networks based on observed links. Because of its broad applications in different domains, link prediction has attracted increasing attention from computer scientists, biologists and physicists recently. Link prediction can be categorized into two classes: one is forecasting the future links, which can be used to help on-line social network users find new friends; the other is determining the hidden or unobserved relationships between nodes, such as protein-protein interaction networks and food webs. The discovery of interaction links in biological networks is usually expensive, therefore, finding the most promising latent links instead of checking all possible links is important in reducing experimental costs.

In the past decade, many works have been done about link prediction in certain graphs, graphs where the network structure is exactly and deterministically known. There are many metrics available for computing the similarity of two nodes. According to the characteristics of these metrics, they can be divided into neighbor-based metrics, path-based metrics, random-walk-based metrics and social theory-based metrics. Furthermore, there are some learning-based methods that have been proposed in recent years.
\subsection{Neighbor-based Metrics}

Among all approaches, neighbor-based metrics are the simplest yet effective to predict missing links. These metrics assume that two nodes are more likely to be connected if they have more common neighbors. Researchers design a lot of neighbor-based metrics for link prediction. Their definitions are as follows:

%\section{Neighbor-based Metrics for Link Prediction}
%\subsection{For certain networks}
\textbf{Common Neighbors (CN)}: Common Neighbors (CN) \cite{newman2001clustering} is the simplest metric among all neighbor-based metrics. It simply counts the number of common neighbors between two nodes and ignores their total number of neighbors. Two nodes, $\mathcal{V}_x$ and $\mathcal{V}_y$, are more likely to form a link if they have many common neighbors. Let $\Gamma(x)$ denote the set of neighbors of node $\mathcal{V}_x$. This measure is defined as follows:
\begin{equation}
s_{xy}=|\Gamma(x)\cap\Gamma(y)|
\end{equation}

CN ignores that different common neighbors have different contributions on the connection likelihood. To solve this problem, other variants such as Resource Allocation and Adamic-Adar metrics are proposed, where a common neighbor with low degree is advocated for by assigning more weight to it. 

\textbf{Resource Allocation (RA)}: Resource Allocation (RA) \cite{zhou2009predicting} metric is regarded as one of the best neighbor-based metrics because of its performance. Considering a pair of nodes, $\mathcal{V}_x$ and $\mathcal{V}_y$, which are not directly connected. The node $\mathcal{V}_x$ can send some resource to $\mathcal{V}_y$, with their common neighbors playing the role of transmitters. In the simplest case, we assume that each transmitter has a unit of resource, and will evenly distribute to all its neighbors. As a results the amount of resource $\mathcal{V}_y$ received is defined as the similarity
between $\mathcal{V}_x$ and $\mathcal{V}_y$, which is:
\begin{equation}
s_{xy}=\sum_{z\in \Gamma(x)\cap\Gamma(y)}\frac{1}{k(z)}
\end{equation}
where $k(z)$ is the degree of node $\mathcal{V}_z$, namely $k(z) = |\Gamma(z)|$

\textbf{Adamic-Adar Coefficient (AA)}: (copy) The AA metric was proposed by Adamic and Adar for computing similarity between two web pages firstly at \cite{adamic2003friends}, subsequent to which it has been widely used in social networks. Similarly to CN, common neighbors which have fewer neighbors are also weighted more heavily. It is defined as:
\begin{equation}
s_{xy}=\sum_{z\in \Gamma(x)\cap\Gamma(y)}\frac{1}{\log{k(z)}}
\end{equation}

(copy) Since CN is not normalized, some neighbor-based metrics also consider how to normalize the CN metric reasonably.

\textbf{Jaccard Coefficient (JC)}: (copy) Jaccard coefficient \cite{jaccard1901etude} normalizes the size of common neighbors. It assumes higher values for pairs of nodes which share a higher proportion of common neighbors relative to total number of neighbors they have. This measure is defined as:
\begin{equation}
s_{xy}=\frac{|\Gamma(x)\cap\Gamma(y)|}{|\Gamma(x)\cup\Gamma(y)|}
\end{equation}

Other similar normalized metrics include:

\textbf{S{\o}rensen Index (SI)} \cite{sorensen1948method}
\begin{equation}
s_{xy}=\frac{|\Gamma(x)\cap\Gamma(y)|}{|\Gamma(x)|+|\Gamma(y)|}
\end{equation}

\textbf{Salton Cosine Similarity (SC)} \cite{salton1986introduction}
\begin{equation}
s_{xy}=\frac{|\Gamma(x)\cap\Gamma(y)|}{\sqrt[]{|\Gamma(x)|\cdot|\Gamma(y)|}}
\end{equation}

Other neighbor-based metrics include Hub Promoted Index (HPI) \cite{ravasz2002hierarchical}, Hub Depressed Index (HDI) \cite{zhou2009predicting}, Leicht-Holme-Newman Index (LHNI) \cite{leicht2006vertex} and Preferential Attachment (PA) \cite{barabasi2002evolution}.
\subsection{Path-based Metrics}
(Copy from review paper.) Besides node and neighbor’s information, paths between two nodes can also be used for computing similarities of node pairs, and we call such methods path-based metrics.

\textbf{Katz Index (KI)}: (From LP paper, it's OK.) Katz Index \cite{katz1953new} is based on the ensemble of all paths, which directly sums over the collection of paths and exponentially damped by length to give the short paths more weights. It is defined as:
\begin{equation}
s_{xy} = \sum_{l=1}^{\infty}\beta \cdot |\text{path}_{x,y}^{l}|
\end{equation}
where $\text{path}_{x,y}^{l}$ is the set of all paths with length $l$ connecting nodes $\mathcal{V}_x$ and $\mathcal{V}_y$, and $\beta$ is a free parameter controlling the weights of the paths. Obviously, a very small $\beta$ yields a measure close to CN because the long paths contribute very little.

\textbf{Local Path (LP)}: (Modified, it's OK.) Unlike Katz Index that considers paths with all possible length. To provide a good trade-off of accuracy and complexity, LP metric \cite{lu2009similarity} only makes use of information of local paths with length 2 and length 3. Obviously, paths of length 2 are more relevant than paths of length 3, so there is an adjustment factor $\alpha$ applied in the measure. $\alpha$ should be a small number close to 0. (If $\alpha=0$, LP is the same as CN.) The metric is defined as \ref{LPEquation}. Here, $A^2$ and $A^3$ denote the number of all paths with length 2 and 3 connecting nodes $\mathcal{V}_x$ and $\mathcal{V}_y$ respectively.
\begin{equation}\label{LPEquation}
s_{xy}=A^2 + \alpha A^3
\end{equation}

\subsection{Random-walk-based Metrics}
(Copy from review paper, add one sentence "the whole ...") Social interactions between nodes in social networks can also be modeled by random walk, which uses transition probabilities from a node to its neighbors to denote the destination of a random walker from current node. The whole process is a Markov chain describing the sequence of nodes visited by a random walker. There exists some link prediction metrics which calculate similarities between nodes based on random walk.

\textbf{Hitting Time (HT)} \cite{gobel1974random} $HT(x, y)$ is the expected number of steps required for a random walk from node $\mathcal{V}_x$ to node $\mathcal{V}_y$. It is defined as follows:
\begin{equation}
HT(x,y) = 1 + \sum_{w\in \Gamma(x)}P_{x,w}HT(w,y) 
\end{equation}
Where $P_{x,w}$ is the probability of stepping on node $\mathcal{V}_w$ from node $\mathcal{V}_x$.

\textbf{Commute Time (CT)}: Since the hitting time metric is not symmetric, commute time is used to count the expected steps both from $\mathcal{V}_x$ to $\mathcal{V}_y$ and from $\mathcal{V}_y$ to $\mathcal{V}_x$. It can be obtained as follows:
\begin{equation}
CT(x,y) = HT(x,y) + HT(y,x)
\end{equation}

\textbf{Local-random-walk-based Index (LRWI)}: Local-random-walk-based Index is based on local random walk, which has lower computational complexity compared with other random-walk-based similarity metrics. It is defined as:
\begin{equation}
s_{xy}=\frac{k(x)}{2|E|}\cdot \pi_{xy}(t) + \frac{k(y)}{2|E|}\cdot \pi_{yx}(t) 
\end{equation}
Here, $|E|$ is the number of links in the network. $k(x)$ is the degree of the node $\mathcal{V}_x$. $\pi_{xy}(t)$ is the probability that a random walker starts from node $\mathcal{V}_x$ and locates at node $\mathcal{V}_y$ after $t$ steps. $t$ is a tunable hyper-parameter. When $t=2$, this metric is the same as RA.

%\textbf{Superposed-random-walk-based Index}
\subsection{Learning-based Algorithms}
\textbf{Local-Naive-Bayes-based Index (LNB)}: Zhen \cite{liu2011link} proposed a probabilistic model called local naive Bayes (LNB) based on the Bayes theory. Different to traditional methods in which each common neighbor contributes equally to the link likelihood, LNB considers that different common neighbors may play different roles in link prediction. The characteristic of the model is that two node pairs with same number of common neighbors could have different connection likelihoods. It is defined as:
\begin{equation}
s_{xy} = \sum_{w\in \Gamma(x)\cap\Gamma(y)}f(k(w))\log(sR_w)
\end{equation}
Here, $s=\frac{M}{M^T}-1$ ($M=\frac{|\mathcal{V}|(|\mathcal{V}|-1)}{2}$, $M^T=|\mathcal{E}|$), $R_w=\frac{N_{\triangle w}+1}{N_{\wedge w}+1}$ ($N_{\triangle w}$ and $N_{\wedge w}$ are respectively the number of connected and disconnected node pairs whose common neighbors include node $\mathcal{V}_w$). There are three forms of function $f$, namely $f(k(w))=1$, $f(k(w))=\frac{1}{\log k(w)}$ and $f(k(w))=\frac{1}{k(w)}$, which are corresponding to the Local Naive Bayes (LNB) form of CN, AA and RA metrics respectively.

\subsection{Link Prediction Algorithms for Uncertain Graphs}

\section{Community Detection}
Community detection algorithms for global community detection.

\subsection{Local Community Detection}
The task of local community detection aims to find a local community for a certain start node. There are some effective approaches to explore local community structure. These methods are listed as follows.

Clauset \cite{clauset2005finding} and Jiyang \cite{chen2009detecting} proposed the local modularity $\mathcal{R}$ for the local community evaluation problem and used $\mathcal{R}$ in the expansion step to find the best local community.
\begin{equation}
\mathcal{R} = \frac{\mathcal{B}_{in\_edge}}{\mathcal{B}_{out\_edge}+\mathcal{B}_{in\_edge}}
\end{equation}
where $\mathcal{B}_{in\_edge}$ is the number of edges that connect boundary nodes and other nodes in $\mathcal{D}$, while $\mathcal{B}_{out\_edge}$ is the number of edges that connect boundary nodes and nodes in $\mathcal{S}$. Thus, $\mathcal{R}$ measures the fraction of those “inside-community” edges in all edges with one or more endpoints in $\mathcal{B}$ and community $\mathcal{D}$ is measured by the sharpness of the boundary given by $\mathcal{B}$.

To find a local community for a start node in deterministic networks. Clauset \cite{clauset2005finding} and Jiyang \cite{chen2009detecting} proposed the local community identification algorithm based on the local modularity $\mathcal{R}$. The algorithm firstly place the start node in the community and its neighbors in the shell node set. At each step, the algorithm adds to the community the neighbor node which gives the largest increase of $\mathcal{R}$. Then the algorithm update the community set, the boundary set, the shell node set and the $\mathcal{R}$ value. This process will not finish until there are no candidate nodes that could increase $\mathcal{R}$.

Similarly, Luo \cite{luo2008exploring} later proposed the modularity $M$ for local community evaluation. Instead of measuring the internal edge fraction of boundary nodes, they directly compare the ratio of internal and external edges. The modularity $M$ is defined as:
\begin{equation}
M = \frac{\text{number of internal edges}}{\text{number of external edges}}
\end{equation}
This algorithm has both an addition step and a deletion step. Nodes will be added or removed from $\mathcal{D}$ and only if it can cause an increase in $M$. This algorithm turns out to result in high recall but low accuracy.

Chen \cite{chen2009local} have presented an alternative method to discover local communities, which aims at reducing outliers and improving detection accuracy. A new measure of local community structure called $L$ was also proposed to help optimize the community hierarchy. The definition of the modularity $L$ is:
\begin{equation}
L=\frac{\frac{\sum_{i\in \mathcal{D}}IK_i}{|\mathcal{D}|}}{\frac{\sum_{j\in \mathcal{B}}EK_j}{|\mathcal{B}|}}
\end{equation}
Here, $IK_i$ is the number of edges between node $\mathcal{V}_i$ and nodes in $\mathcal{D}$, and $EK_j$ is the number of connections between node $\mathcal{V}_j$ and nodes in $\mathcal{S}$.

Due to its strict criteria in agglomerating nodes, this algorithm can hardly obtain a comparatively integrated community structure.

\end{document}