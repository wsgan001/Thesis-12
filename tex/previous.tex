% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{A Review of Complex Network Analysis}
In this Chapter, we review some of the related work relevant to this thesis. This thesis is mainly about the problems of link prediction, local community detection and entity ranking in uncertain networks. In certain networks, these problems have been fully studied, so we first review the existing algorithms in certain scenarios. There are also algorithms about complex network analysis in uncertain scenarios, we also summarize them briefly.
\section{Link Prediction}\label{previous:link-prediction}
%Link prediction is the problem of determining future or missing associations between entities in complex networks based on observed links. Because of its broad applications in different domains, link prediction has attracted increasing attention from computer scientists, biologists and physicists recently. Link prediction can be categorized into two classes: one is forecasting the future links, which can be used to help on-line social network users find new friends; the other is determining the hidden or unobserved relationships between nodes, such as protein-protein interaction networks and food webs. The discovery of interaction links in biological networks is usually expensive, therefore, finding the most promising latent links instead of checking all possible links is important in reducing experimental costs.

In the past decade, many works have been done about link prediction in certain graphs, graphs where the network structure is exactly and deterministically known. There are many metrics available for computing the similarity of two nodes. According to the characteristics of these metrics, they can be divided into neighbor-based metrics, path-based metrics, random-walk-based metrics and social theory-based metrics. Furthermore, there are some learning-based methods that have been proposed in recent years.
\subsection{Neighbor-based Metrics}

Among all approaches, neighbor-based metrics are the simplest yet effective to predict missing links. These metrics assume that two nodes are more likely to be connected if they have more common neighbors. Researchers design a lot of neighbor-based metrics for link prediction. Their definitions are as follows:

%\section{Neighbor-based Metrics for Link Prediction}
%\subsection{For certain networks}
\textbf{Common Neighbors (CN)}: Common Neighbors (CN) \cite{newman2001clustering} is the simplest metric among all neighbor-based metrics. It simply counts the number of common neighbors between two nodes and ignores their total number of neighbors. Two nodes, $\mathcal{V}_x$ and $\mathcal{V}_y$, are more likely to form a link if they have many common neighbors. Let $\Gamma(x)$ denote the set of neighbors of node $\mathcal{V}_x$. This measure is defined as follows:
\begin{equation}
s_{xy}=|\Gamma(x)\cap\Gamma(y)| \label{CN-previous}
\end{equation}

CN ignores that different common neighbors have different contributions on the connection likelihood. To solve this problem, other variants such as Resource Allocation and Adamic-Adar metrics are proposed, where a common neighbor with low degree is advocated for by assigning more weight to it. 

\textbf{Resource Allocation (RA)}: Resource Allocation (RA) \cite{zhou2009predicting} metric is regarded as one of the best neighbor-based metrics because of its performance. Considering a pair of nodes, $\mathcal{V}_x$ and $\mathcal{V}_y$, which are not directly connected. The node $\mathcal{V}_x$ can send some resource to $\mathcal{V}_y$, with their common neighbors playing the role of transmitters. In the simplest case, we assume that each transmitter has a unit of resource, and will evenly distribute to all its neighbors. As a results the amount of resource $\mathcal{V}_y$ received is defined as the similarity
between $\mathcal{V}_x$ and $\mathcal{V}_y$, which is:
\begin{equation}
s_{xy}=\sum_{z\in \Gamma(x)\cap\Gamma(y)}\frac{1}{k(z)}
\end{equation}
where $k(z)$ is the degree of node $\mathcal{V}_z$, namely $k(z) = |\Gamma(z)|$

\textbf{Adamic-Adar Coefficient (AA)}: The AA metric was proposed by Adamic and Adar for computing similarity between two web pages firstly at \cite{adamic2003friends}, subsequent to which it has been widely used in social networks. Similarly to CN, common neighbors which have fewer neighbors are also weighted more heavily. It is defined as:
\begin{equation}
s_{xy}=\sum_{z\in \Gamma(x)\cap\Gamma(y)}\frac{1}{\log{k(z)}}
\end{equation}

Since CN is not normalized, some neighbor-based metrics also consider how to normalize the CN metric reasonably.

\textbf{Jaccard Coefficient (JC)}: Jaccard coefficient \cite{jaccard1901etude} normalizes the size of common neighbors. It assumes higher values for pairs of nodes which share a higher proportion of common neighbors relative to total number of neighbors they have. This measure is defined as:
\begin{equation}
s_{xy}=\frac{|\Gamma(x)\cap\Gamma(y)|}{|\Gamma(x)\cup\Gamma(y)|}
\end{equation}

Other similar normalized metrics include:

\textbf{S{\o}rensen Index (SI)} \cite{sorensen1948method}
\begin{equation}
s_{xy}=\frac{|\Gamma(x)\cap\Gamma(y)|}{|\Gamma(x)|+|\Gamma(y)|}
\end{equation}

\textbf{Salton Cosine Similarity (SC)} \cite{salton1986introduction}
\begin{equation}
s_{xy}=\frac{|\Gamma(x)\cap\Gamma(y)|}{\sqrt[]{|\Gamma(x)|\cdot|\Gamma(y)|}}
\end{equation}

Other neighbor-based metrics include Hub Promoted Index (HPI) \cite{ravasz2002hierarchical}, Hub Depressed Index (HDI) \cite{zhou2009predicting}, Leicht-Holme-Newman Index (LHNI) \cite{leicht2006vertex} and Preferential Attachment (PA) \cite{barabasi2002evolution}.
\subsection{Path-based Metrics}
Besides node and neighbor’s information, paths between two nodes can also be used for computing similarities of node pairs, and we call such methods path-based metrics.

\textbf{Katz Index (KI)}: Katz Index \cite{katz1953new} is based on the ensemble of all paths, which directly sums over the collection of paths and exponentially damped by length to give the short paths more weights. It is defined as:
\begin{equation}
s_{xy} = \sum_{l=1}^{\infty}\beta \cdot |\text{path}_{x,y}^{l}|
\end{equation}
where $\text{path}_{x,y}^{l}$ is the set of all paths with length $l$ connecting nodes $\mathcal{V}_x$ and $\mathcal{V}_y$, and $\beta$ is a free parameter controlling the weights of the paths. Obviously, a very small $\beta$ yields a measure close to CN because the long paths contribute very little.

\textbf{Local Path (LP)}: Unlike Katz Index that considers paths with all possible length. To provide a good trade-off of accuracy and complexity, LP metric \cite{lu2009similarity} only makes use of information of local paths with length 2 and length 3. Obviously, paths of length 2 are more relevant than paths of length 3, so there is an adjustment factor $\alpha$ applied in the measure. $\alpha$ should be a small number close to 0. (If $\alpha=0$, LP is the same as CN.) The metric is defined as \ref{LPEquation}. Here, $A^2$ and $A^3$ denote the number of all paths with length 2 and 3 connecting nodes $\mathcal{V}_x$ and $\mathcal{V}_y$ respectively.
\begin{equation}\label{LPEquation}
s_{xy}=A^2 + \alpha A^3
\end{equation}

\subsection{Random-walk-based Metrics}
Social interactions between nodes in social networks can also be modeled by random walk, which uses transition probabilities from a node to its neighbors to denote the destination of a random walker from current node. The whole process is a Markov chain describing the sequence of nodes visited by a random walker. There exists some link prediction metrics which calculate similarities between nodes based on random walk.

\textbf{Hitting Time (HT)}: \cite{gobel1974random} $HT(x, y)$ is the expected number of steps required for a random walk from node $\mathcal{V}_x$ to node $\mathcal{V}_y$. It is defined as follows:
\begin{equation}
HT(x,y) = 1 + \sum_{w\in \Gamma(x)}P_{x,w}HT(w,y) 
\end{equation}
Where $P_{x,w}$ is the probability of stepping on node $\mathcal{V}_w$ from node $\mathcal{V}_x$.

\textbf{Commute Time (CT)}: Since the hitting time metric is not symmetric, commute time is used to count the expected steps both from $\mathcal{V}_x$ to $\mathcal{V}_y$ and from $\mathcal{V}_y$ to $\mathcal{V}_x$. It can be obtained as follows:
\begin{equation}
CT(x,y) = HT(x,y) + HT(y,x)
\end{equation}

\textbf{Local-random-walk-based Index (LRWI)}: Local-random-walk-based Index is based on local random walk, which has lower computational complexity compared with other random-walk-based similarity metrics. It is defined as:
\begin{equation}
s_{xy}=\frac{k(x)}{2|E|}\cdot \pi_{xy}(t) + \frac{k(y)}{2|E|}\cdot \pi_{yx}(t) 
\end{equation}
Here, $|E|$ is the number of links in the network. $k(x)$ is the degree of the node $\mathcal{V}_x$. $\pi_{xy}(t)$ is the probability that a random walker starts from node $\mathcal{V}_x$ and locates at node $\mathcal{V}_y$ after $t$ steps. $t$ is a tunable hyper-parameter. When $t=2$, this metric is the same as RA.

%\textbf{Superposed-random-walk-based Index}
\subsection{Learning-based Algorithms}
\textbf{Local-Naive-Bayes-based Index (LNB)}: Zhen \cite{liu2011link} proposed a probabilistic model called local naive Bayes (LNB) based on the Bayes theory. Different to traditional methods in which each common neighbor contributes equally to the link likelihood, LNB considers that different common neighbors may play different roles in link prediction. The characteristic of the model is that two node pairs with same number of common neighbors could have different connection likelihoods. It is defined as:
\begin{equation}
s_{xy} = \sum_{w\in \Gamma(x)\cap\Gamma(y)}f(k(w))\log(sR_w)
\end{equation}
Here, $s=\frac{M}{M^T}-1$ ($M=\frac{|\mathcal{V}|(|\mathcal{V}|-1)}{2}$, $M^T=|\mathcal{E}|$), $R_w=\frac{N_{\triangle w}+1}{N_{\wedge w}+1}$ ($N_{\triangle w}$ and $N_{\wedge w}$ are respectively the number of connected and disconnected node pairs whose common neighbors include node $\mathcal{V}_w$). There are three forms of function $f$, namely $f(k(w))=1$, $f(k(w))=\frac{1}{\log k(w)}$ and $f(k(w))=\frac{1}{k(w)}$, which are corresponding to the Local Naive Bayes (LNB) form of CN, AA and RA metrics respectively.

\subsection{Link Prediction Algorithms for Weighted Graphs}
The above-mentioned similarity metrics only consider the binary relations among nodes; however, in the real world, links are naturally weighted, which may represent the amount of traffic load along connections in a transportation network or the number of co-authorized papers in a co-authorship network. Murata and Moriyasu \cite{murata2007link} proposed weighted similarity metrics as variants of Common Neighbors, Resource Allocation and Adamic-Adar. The definition of weighted metrics can also be checked in \cite{lu2010link}:

%\subsection{For weighted networks}
\textbf{Weighted Common Neighbors (WCN)}:
\begin{equation}
s_{xy}=\sum_{z\in \Gamma(x)\cap\Gamma(y)}w(x,z)+w(y,z) \label{WCN-previous}
\end{equation}

\textbf{Weighted Resource Allocation (WRA)}:
\begin{equation}
s_{xy}=\sum_{z\in \Gamma(x)\cap\Gamma(y)}\frac{w(x,z)+w(y,z)}{s(z)}
\end{equation}

\textbf{Weighted Adamic-Adar (WAA)}:
\begin{equation}
s_{xy}=\sum_{z\in \Gamma(x)\cap\Gamma(y)}\frac{w(x,z)+w(y,z)}{\log(1+s(z))}
\end{equation}

Here, $w(x, y) = w(y, x)$ denotes the weight of the link between nodes $\mathcal{V}_x$ and $\mathcal{V}_y$, and $s(x)=\sum_{z\in\Gamma(x)}w(x,z)$ is the strength of node $\mathcal{V}_x$.

\subsection{Link Prediction Algorithms for Uncertain Graphs}
Mallek \cite{mallek2016evidential} proposed to use belief function theory to deal with uncertain social networks. Belief function network is considered as a generalization of the probability theory, and one of its uses is the representation and management of missing information. It provides tools for combining of evidence induced from several pieces of information. The approach proposed by Mallek combines sampling techniques and information fusion and returns good results in real-life settings. It used popular structural measures based on local graph information to compute distances between the links, and a fusion procedure was subsequently applied taking into account the reliability of the sources to predict missing links.

Ahmed \cite{ahmed2016efficient} investigated the problem of link prediction in dynamic uncertain networks, he designed a new method based on a random walk in temporal uncertain networks. His method first transformed the link prediction problem in uncertain networks to a random walk in a deterministic network. Then, the similarity scores between a node and its neighbors were calculated within a sub-graph around this node to reduce the computational time. He also extended the method for solving link prediction in temporal uncertain networks. The proposed method integrated time and global topological information and obtained accurate results.

\newpage

\section{Community Detection}
In the past 15 years, a large number of community detection detection algorithms have been proposed for certain graphs. According to the characteristics of these algorithms, they can be divided into graph partitioning-based algorithms \cite{kernighan1970efficient,newman2013community}, clustering-based algorithms \cite{girvan2002community,newman2004fast,blondel2008fast,clauset2004finding}, genetic algorithms-based algorithms \cite{pizzuti2008ga} and label propagation-based algorithms \cite{raghavan2007near}. In this part, we mainly review graph partitioning-based algorithms and clustering-based algorithms, besides, we also review some local community detection algorithms.

\subsection{Graph Partitioning-based Algorithms}
Graph partitioning is the process of partitioning a graph into a predefined number of smaller components with specific properties. A common property to be minimized is called cut size. A cut is a partition of the vertex set of a graph into two disjoint subsets, and the size of the cut is the number of edges between the components. A multicut is a set of edges whose removal divides the graph into two or more components. It is necessary to specify the number of components one wishes to get in case of graph partitioning. There is a long tradition of research by computer scientists on graph partitioning, and a wide variety of heuristic algorithms have been developed that give acceptable good solutions in many cases.

The Kernighan-Lin \cite{kernighan1970efficient} algorithm is one of the earliest methods proposed and is still frequently used, often in combination with other techniques. It partitions the nodes of the graph into subsets of given sizes so as to minimize the sum of costs on all edges cut. In each pass, the algorithm improves on a division of the network by optimizing of the number of within- and between-community edges using a greedy algorithm. Given a graph with $n$ nodes, each pass of the algorithm runs in time $O(n^2 \log n)$. A major disadvantage of this algorithm is that the number of groups has to be predefined, but we do not know how many communities there are, and there is no reason that they should be roughly the same size.

Another popular technique is the spectral bisection method \cite{barnes1982algorithm}, which is based on the properties of the spectrum of the Laplacian matrix. The spectral bisection method is quite fast, and it gives in general good partitions, that can be further improved by applying the Kernighan–Lin algorithm.

\subsection{Hierarchical Clustering-based Algorithms}
Clustering is the process of grouping a set of similar items together in structures known as clusters. The hierarchical clustering and partitioning method of clustering are the commonly used clustering techniques that have been discussed in the literature. Here, we mainly focus on hierarchical clustering-based algorithms. In hierarchical clustering, a hierarchy of clusters is formed. The process of hierarchy creation or leveling can be agglomerative or divisive. In agglomerative clustering methods, a bottom-up approach to clustering is followed. A particular node is clubbed or agglomerated with similar nodes to form a cluster or a community. This aggregation is based on similarity. In divisive clustering approaches, a large cluster is repeatedly divided into smaller clusters.

Girvan and Newman \cite{girvan2002community} proposed a divisive algorithm based on edge-betweenness for a graph with undirected and unweighted edges. The algorithm detects communities by progressively removing edges from the original network. The connected components of the remaining network are the communities. Instead of trying to construct a measure that tells us which edges are the most central to communities, the Girvan-Newman algorithm focuses on edges that are most likely ''between" communities. To find those edges, the algorithm extends the definition of node betweenness to the case of edges, defining the ''edge betweenness" of an edge as the number of shortest paths between pairs of nodes that run along it. The edges connecting communities will have high edge betweenness. By removing these edges, the groups are separated from one another and so the underlying community structure of the network is revealed.

To measure the quality of a particular division of a network, Newman and Girvan \cite{newman2004finding} first defined a measure known as modularity. The modularity was defined as $Q=\sum_ie_{ii}-a_i^2$, where $e_{ij}$ denotes the fraction of all edges in the network that link nodes in community $i$ to nodes in community $j$; while $a_i=\sum_je_{ij}$, which represents the fraction of edges connect to nodes in community $i$. The value $Q = 1$ indicates a network with strong community structure. Later many researchers proposed clustering-based community detection methods based on the optimization of modularity $Q$.

Newman \cite{newman2004fast} has worked to maximize modularity so that the process of aggregating nodes to form communities leads to maximum modularity gain. The algorithm starts with each node in a separate community on its own and amalgamates communities in pairs, choosing at each step the pair whose amalgamation gives the greatest increase in Q.

Clauset \cite{clauset2004finding} used greedy optimization of modularity to detect communities for large networks. For a network structure with $m$ edges and $n$ nodes, the algorithm has a running time of $O(md \log n)$, where $d$ denotes the depth of the dendrogram. For sparse real-world networks, the running time is $O(n\log n)$.

Blondel \cite{blondel2008fast} proposed a heuristic method known as the Louvain method. The algorithm is divided in two phases that are repeated iteratively. First, all nodes are placed into different communities. So, in the initial partition there are as many communities as there are nodes. Then for each node $\mathcal{V}_i$, the change in modularity is calculated for removing $\mathcal{V}_i$ from its own community and moving it into the community of each neighbor $\mathcal{V}_j$ of $\mathcal{V}_i$. Once this value is calculated for all communities $\mathcal{V}_i$ is connected to, $\mathcal{V}_i$ is then placed in the community for which this gain is maximum (in case of a tie we use a breaking rule), but only if this gain is positive. If no positive gain is possible, $\mathcal{V}_i$ stays in its original community. This process is applied repeatedly and sequentially for all nodes until no further improvement can be achieved and the first phase is then complete. In the second phase of the algorithm, it groups all of the nodes in the same community and builds a new network where nodes are the communities from the previous phase. Any links between nodes of the same community are now represented by self loops on the new community node and links from multiple nodes in the same community to a node in a different community are represented by weighted edges between communities. Once the new network is created, the second phase has ended and the first phase can be re-applied to the new network. In Louvain algorithm, the modularity is defined as: 
\begin{equation}
Q=\frac{1}{2m}\sum_{ij}[A_{ij}-\frac{k_ik_j}{2m}]\delta(c_i,c_j)
\end{equation}
where $A_{ij}$ represents the edge weight between nodes $\mathcal{V}_i$ and $\mathcal{V}_j$; $k_{i}$ and $k_j$ are the sum of the weights of the edges attached to nodes $\mathcal{V}_i$ and $\mathcal{V}_j$ respectively; $m$ is the sum of all of the edge weights in the graph; $c_{i}$ and $c_{j}$ are the communities of the nodes; and $\delta$ is a simple delta function.
%\subsection{Partitional Clustering-based Algorithms}

\subsection{Local Community Detection Algorithms} \label{Local-Community-Detection-Algorithms-Review}
The task of local community detection aims to find a local community for a certain start node. Numerous local community detection algorithms have been proposed. Many of these algorithms can be grouped as greedy community expansion, which also provides the basis of other (more complex) algorithms. These methods are listed as follows.

Clauset \cite{clauset2005finding} and Chen \cite{chen2009detecting} proposed the local modularity $\mathcal{R}$ for the local community evaluation problem and used $\mathcal{R}$ in the expansion step to find the best local community.
\begin{equation}
\mathcal{R} = \frac{\mathcal{B}_{in\_edge}}{\mathcal{B}_{out\_edge}+\mathcal{B}_{in\_edge}}
\end{equation}
where $\mathcal{B}_{in\_edge}$ is the number of edges that connect boundary nodes and other nodes in $\mathcal{D}$, while $\mathcal{B}_{out\_edge}$ is the number of edges that connect boundary nodes and nodes in $\mathcal{S}$. Thus, $\mathcal{R}$ measures the fraction of those “inside-community” edges in all edges with one or more endpoints in $\mathcal{B}$ and community $\mathcal{D}$ is measured by the sharpness of the boundary given by $\mathcal{B}$.

To find a local community for a start node in deterministic networks. Clauset \cite{clauset2005finding} and Chen \cite{chen2009detecting} proposed the local community identification algorithm based on the local modularity $\mathcal{R}$. The algorithm firstly place the start node in the community and its neighbors in the shell node set. At each step, the algorithm adds the neighbor node which gives the largest increase of $\mathcal{R}$ to the community. Then the algorithm update the community set, the boundary set, the shell node set and the $\mathcal{R}$ value. This process will not finish until there are no candidate nodes that could increase $\mathcal{R}$.

Similarly, Luo \cite{luo2008exploring} proposed the modularity $M$ for local community evaluation. Instead of measuring the internal edge fraction of boundary nodes, they directly compare the ratio of internal and external edges. The modularity $M$ is defined as:
\begin{equation}
M = \frac{\text{number of internal edges}}{\text{number of external edges}}
\end{equation}
This algorithm has both an addition step and a deletion step. Nodes will be added or removed from $\mathcal{D}$ only if it can cause an increase in $M$. This algorithm turns out to result in high recall but low accuracy.

Chen \cite{chen2009local} later presented an alternative method to discover local communities, which aims at reducing outliers and improving detection accuracy. A new measure of local community structure called $L$ was also proposed to help optimize the community hierarchy. The definition of the modularity $L$ is:
\begin{equation}
L=\frac{\frac{\sum_{i\in \mathcal{D}}IK_i}{|\mathcal{D}|}}{\frac{\sum_{j\in \mathcal{B}}EK_j}{|\mathcal{B}|}}
\end{equation}
Here, $IK_i$ is the number of edges between node $\mathcal{V}_i$ and nodes in $\mathcal{D}$, and $EK_j$ is the number of connections between node $\mathcal{V}_j$ and nodes in $\mathcal{S}$. However, this algorithm can hardly obtain a comparatively integrated community structure due to its strict criteria in agglomerating nodes

For weighted graphs, Jianbin \cite{huang2011towards} defined structure similarity $s(u,v)$ between two adjacent nodes $\mathcal{V}_u$ and $\mathcal{V}_v$ as:
\begin{equation}
s(u,v)=\frac{\sum_{x\in \Gamma(u)\cap\Gamma(v)}w(u,x)\cdot w(v,x)}{\sqrt[]{\sum_{x\in \Gamma(u)}w^2(w,x)}\cdot\sqrt[]{\sum_{x\in \Gamma(v)}w^2(v,x)}}
\end{equation}
When we consider an unweighted graph, the weight $w(u,v)$ of any edge can be set to 1. Based on it, in the agglomeration phase, the candidate node with the largest similarity value will be considered for adding to the community. To check whether a node can be added to the community, Jianbin defined a new measure called Tunable Tightness Gain. Its definition can be found in \cite{huang2011towards}. If the node with the largest similarity value can give positive value of the Tunable Tightness Gain, it will be added to the community.

Wu \cite{wu2012local} proposed a three-phase algorithm. In the agglomeration phase, the node with the highest link similarity value will first be considered for adding to the community. In the optimization phase, all the nodes in boundary will be judged to determine whether they should be removed from the community. After these two phases, a trimming phase is performed in order to remove the outliers. Though this algorithm ensures that the result is a strong community, a lot of nodes will be reported as outliers in optimization phase and trimming phase.

There are also algorithms that work with a different strategy, which is not based on greedy community expansion. One of these is PageRank-Nibble \cite{andersen2006local}, which works by locally approximating PageRank-vectors. It is a two-step algorithm: first, it approximates personalized PageRank vectors around the seed node and sorts all nodes with a positive score according to that score in decreasing order. Then it considers all communities that are a prefix of this sorted list and returns the prefix with minimum conductance as community.

\subsection{Community Detection Algorithms for Uncertain Graphs}
To deal with uncertain networks, one simple idea is thresholding: we assume that edges exist whenever their probability exceeds a certain threshold that we choose. Krogan et al. \cite{krogan2006global} first converted the uncertain network into a conventional binary network based on this idea, then applied traditional community detection algorithms to the conventional binary network. While this technique can certainly reveal useful information, it has some drawbacks. First, there is the issue of the choice of the threshold level. Krogan et al. used a value of 0.273 for their threshold, but there is little doubt that their results would be different if they had chosen a different value, and we have the question what threshold should we choose if we have a totally different uncertain network. Second, thresholding throws away potentially useful information. There is a substantial difference between an edge with probability 0.3 and an edge with probability 0.9, but the distinction is lost if one applies a threshold at 0.273.

Dahlin et al. \cite{dahlin2011method} proposed a method which is based on sampling. The method mainly has three steps. Firstly, he samples candidate networks from the ensemble of certain networks that are consistent with the available information about the uncertain networks using Monte Carlo methods. Then, standard community detection methods can be applied to candidate networks. Lastly, he merges candidate communities into the most probable community structure of the uncertain network. This method also has its shortcomings. To get reliable communities, one needs to sample enough candidate networks, which will result in high computation complexity.

Liu et al. \cite{liureliable} proposed a generalized reliability criterion from two basic intuitions (purity and size balance) to overcome the challenges from standard reliability criterion, and developed a novel k-means algorithm to solve the uncertain clustering problem. The criterion is designed from an information-theoretic perspective, and the use of such a criterion enables the design of an extremely simple and efficient version of the k-means algorithm, while retaining the desired qualitative properties.

Martin et al. \cite{martin2016structural} described a method for performing the common task of community detection on uncertain networks by fitting a generative network model to the data using a combination of an expectation maximization (EM) algorithm and belief propagation. They also shown how the resulting fit can be used to reconstruct the true underlying network by making predictions of which nodes are connected by edges. 

\newpage

\section{Centrality and Rank}
A measure of centrality on a graph aims to assign a ranking or magnitude to each node that captures the relative importance of that node in the context of the graph’s structure. Here are some of the key centrality measures from the literature.
\subsection{Degree Centrality}
Degree centrality is one of the simplest centrality measures. It is defined as the number of edges a node $\mathcal{V}_i$ has, $deg(i)$. It can be normalized by the maximal possible degree, $n-1$, to obtain a number between 0 and 1:
\begin{equation}
c^{deg}(i) = \frac{deg(i)}{n-1}
\end{equation}
The degree centrality is a measure of the size of $\mathcal{V}_i$'s immediate network. It gives some insight into the popularity of the node $\mathcal{V}_i$, but misses potentially important aspects of the whole architecture of the network and a node’s position in the network.
\subsection{Closeness Centrality}
Closeness centrality \cite{freeman1978centrality} is based on the network distance between a node and each other reachable node. It can be regarded as a measure of how long it will take to spread information from a start node to all other nodes sequentially. A path from $\mathcal{V}_i$ to $\mathcal{V}_j$ is called a shortest path if it minimizes the number of steps in the sequence, and the distance from $\mathcal{V}_i$ to $\mathcal{V}_j$, denoted $d(i,j)$, is the number of steps in such a shortest path from $\mathcal{V}_i$ to $\mathcal{V}_j$. In a connected graph, the closeness centrality of $\mathcal{V}_i$ is defined as the reciprocal of the average shortest path distance to $\mathcal{V}_i$ over all $n-1$ reachable nodes.
\begin{equation}
c^{clo}(i) = \frac{n-1}{\sum_{i\neq j}d(i,j)}
\end{equation}
When a graph is not strongly connected, Wasserman and Faust \cite{wasserman1994social} proposed an improved formula, which is a ratio of the fraction of nodes in the network which are reachable, to the average distance from the reachable nodes. It is defined as:
\begin{equation}
c^{clo}(i) = \frac{n-1}{N-1}\frac{n-1}{\sum_{i\neq j}d(i,j)}
\end{equation}
where $N$ is the total number of nodes in the network.

\subsection{Betweenness Centrality}
Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. It was introduced as a measure for quantifying the control of a human on the communication between other humans in a social network by Linton Freeman \cite{freeman1977set}. It captures the role of a node as an intermediary in the transmission of information or resources between other nodes in the network. Nodes that have a high probability to occur on a randomly chosen shortest path between two randomly chosen nodes have a high betweenness. The betweenness centrality can be represented as:
\begin{equation}
c^{bet}(i) = \sum_{s\neq v \neq t}\frac{\sigma_{st}(v)}{\sigma_{st}}
\end{equation}
where $\sigma_{st}$ is the total number of shortest paths from node $\mathcal{V}_s$ to node $\mathcal{V}_t$ and $\sigma_{st}(v)$ is the number of those paths that pass through $\mathcal{V}_v$.

%(What about disconnected network?)
\subsection{Eigenvector Centrality}
Eigenvector centrality \cite{bonacich1987power} is a measure of the influence of a node in a network. It computes the centrality for a node based on the centrality of its neighbors. It assumes that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. The relative centrality score of node $\mathcal{V}_i$ can be defined as:
\begin{equation}
c^{eig}(i) = \frac{1}{\lambda}\sum_{\mathcal{V}_j\in \Gamma(i)}c^{eig}(j)
\end{equation}
where $\Gamma(i)$ is a set of the neighbors of $\mathcal{V}_i$ and $\lambda$ is a constant. It can also be rewritten in vector notation as the eigenvector equation:
\begin{equation}
AC^{eig}=\lambda C^{eig}
\end{equation}
where $C^{eig} = (c^{eig}(1),c^{eig}(2),...,c^{eig}(n))$; $A=(a_{i,j})$ is the adjacency matrix, i.e. $a_{i,j}=1$ if node $\mathcal{V}_i$ is linked to node $\mathcal{V}_j$, and $a_{i,j}=0$ otherwise.

The most famous variant of eigenvector centrality is Google's PageRank algorithm \cite{page1999pagerank}: webpages rank highly in Google’s search results if they are linked from other webpages of high rank. It is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known.

\subsection{Centrality Measures in Uncertain Graphs} \label{CMIUG} 
In order to model the effect of relationship uncertainty on network connectivity, Sevon et al. \cite{sevon2006link} developed a method for biological databases. They proposed to transform the probabilities into weights by taking the negative logarithm of the probabilities:
\begin{equation}
w(e) = -\log(p(e))
\end{equation}
Then standard algorithms for finding shortest paths can be applied. Closeness centrality and betweenness centrality can also be calculated based on shortest paths.

Pfeiffer and Neville \cite{pfeiffer2010probabilistic} formulated a measure of centrality based on the most probable paths of communication, rather than shortest paths. They developed a notion of probabilistic paths in uncertain networks, and used it as a foundation for computing probabilistic betweenness centrality in networks evolving over time. The motivation behind this approach is as follows: Conventional betweenness centrality uses shortest paths as an indication of how quickly information can potentially flow in the network. When adopting a probabilistic view of the network, information flowing across paths with fewer nodes is less important than whether the information is successfully transmitted. In this case, central nodes should correspond to nodes that have high probability of transferring information throughout the graph, regardless of path length.


\end{document}