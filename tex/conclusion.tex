% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\onlyinsubfile{\externaldocument{\main/tex/introduction}}

\begin{document}

\chapter{Conclusion}
In this work, we focus on the problem of local community detection with \textit{edge uncertainty}. We propose a way to convert the uncertain community detection problem into the deterministic scenario. Then we illustrate with an example that periphery nodes tend to be grouped into their neighbor communities in uncertain networks, and we propose a new measure $\mathcal{K}$ to tackle this problem. Due to the very limited publicly available uncertain network datasets, we also put forward a way to generate uncertain networks. Finally, we evaluate our algorithm in supervised and unsupervised ways.

\section{Introduction}
Many datasets can be represented by networks consisting of a set of nodes and edges connecting these edges. Examples include protein-protein interaction networks \cite{krogan2006global}, food webs \cite{williams2000simple}, social networks \cite{shetty2004enron}, air transportation networks, collaboration networks \cite{nascimento2003analysis,leskovec2007graph} and the worldwide web \cite{albert1999internet,broder2000graph}. The nodes in many networks fall naturally into groups or communities. Nodes in the same community are densely connected, while the number of edges between nodes of different communities is much smaller. Community detection is the problem of finding such communities in complex networks based on edges between nodes. Detection of these communities is key to understanding the structure of complex networks and extracting useful information from them. The discovery of communities in networks can be useful in various applications. For example, detection of groups within the worldwide web can be used to find sets of web pages on related topics \cite{flake2002self}; detection of groups within social networks can also be used to find social units or communities \cite{girvan2002community}. Besides, community detection can also be used in analyzing trend in citation networks \cite{bedi2016community} and improving recommender systems \cite{cao2015improved}. Because of its broad applications in different domains, community detection has attracted increasing attention from computer scientists, biologists and physicists recently. 

In the past 15 years, a large number of community detection detection algorithms have been proposed for certain graphs, graphs where the network structure is exactly and deterministically known. According to the characteristics of these algorithms, they can be divided into graph partitioning-based algorithms \cite{kernighan1970efficient,newman2013community}, clustering-based algorithms \cite{girvan2002community,newman2004fast,blondel2008fast,clauset2004finding}, genetic algorithms (GA)-based algorithms \cite{pizzuti2008ga} and label propagation-based algorithms \cite{raghavan2007near}.

Most previous approaches on community detection have focused on networks where the structure is exactly known, and they require knowledge of the entire graph structure to identify communities. When we run these algorithms. we need to access and see the whole network information. However, recently we are faced with two problems. The first problem is that we have increasing number of networks which have edge uncertainty, which means the network structure is not exactly and deterministically known. The edges are constructed through uncertain or statistical inference, so we only know the connections between nodes with a certain probability. Examples of such networks include protein-protein interaction networks with experimentally inferred links, sensor networks with uncertain connectivity links, or social networks, which are augmented with inferred friendship, similarity, or trust links. Another problem of previous methods is that the requirement of accessing the whole network information can not be satisfied when networks become too large to know completely, e.g., the WWW. In this scenario, it is hard to identify global communities, however, finding local community for a certain node is still useful. A local community is a community defined based on local information without having access to the entire network. For instance, we may want to quantify local community of a person given his social network in Facebook.

To solve the first problem, Krogan \cite{krogan2006global} converted the uncertain network into a conventional binary network by thresholding the likelihoods. Johan \cite{dahlin2011method} proposed a method which is based on sampling from an ensemble of certain networks that are consistent with the available information about the uncertain networks. Lin \cite{liureliable} developed a novel k-means algorithm to solve the uncertain clustering problem. Travis \cite{martin2016structural} gave a principled maximum-likelihood method for inferring community structure. To solve the second problem, several different local modularity metrics \cite{clauset2005finding,chen2009detecting,chen2009local} have been proposed to identify local community structure given limited information. These methods aimed to solve one of the two aforementioned problems. However, the two problems may appear simultaneously in some scenarios. Up to now, there is no algorithm which is able to tackle both problems at the same time.

In this work, we mainly focus on the problem of local community detection in the context of uncertain networks. We propose a way to convert the uncertain community detection problem into the deterministic scenario, and put forward a new measure to tackle the problem existing in uncertain networks. The remainder of this paper is organized as follows. In Section 2, we provide the problem definition. In Section 3, we review related works. In Section 4, we show the limitation of previous works and develop a new algorithm to find local communities in uncertain networks. In Section 5, we present our evaluation metrics and the experiment results. We also demonstrate how we find the best hyper-parameter in this section. Finally, we conclude the results in Section 6.

\section{Problem Definition}
\subsection{Uncertain Network}
An uncertain graph $\mathcal{G = (V,E,P)}$ is defined over a set of nodes $\mathcal{V}$, a set of edges $\mathcal{E}$, and a set of probabilities $\mathcal{P}$ of edge existence. Note the probability over the edge between node $\mathcal{V}_i$ and node $\mathcal{V}_j$ can be represented as $\mathcal{P}_{i,j}$ or $\mathcal{P}_{j,i}$. The multiple links and self-connections are not allowed. Figure \ref{uncertain_network_example} is an example of an uncertain network.
\begin{figure}
\centering
\includegraphics[scale = 0.25]{\main/img/probabilistic_graph.pdf}
\caption{An example of uncertain network. The numbers are existential probabilities.}
\label{uncertain_network_example}
\end{figure}
\subsection{Local Community Detection}
As mentioned in the introduction, local communities are densely-connected node sets which are discovered and evaluated based only on local information. The task of local community detection aims to find a local community for a certain start node. Clauset, Jiyang and Wu proposed local community detection problem settings for certain networks in \cite{clauset2005finding,chen2009detecting,wu2012local}. Here, we reiterate them in the context of uncertain networks.

Suppose that in an undirected network $\mathcal{G}$, we start with one node and we know all its possible neighbors and the possibilities of edge existence between the start node and all possible neighbors. Note we use ``possible" here because there exists uncertainty in the existence of edges between start node and neighbors. The possibility is in the range of (0,1]. We use $\mathcal{D}$ to denote the known local community of the graph (for the start node). This necessarily implies that we also have limited information for another shell node set $\mathcal{S}$, which contains nodes that are possible neighbors of nodes in $\mathcal{D}$ but do not belong to $\mathcal{D}$ (note ``limited" means nodes in $\mathcal{S}$ may also have other possible neighbors that are not in $\mathcal{D}$, but we do not know this information until we visit them). In such circumstances, the only way to gain additional information about the network $\mathcal{G}$ is to visit possible neighbor nodes $s_i$ of $\mathcal{D}$ (where $s_i\in \mathcal{S}$) and obtain the possible neighbors of $s_i$ and the possibilities of edge existence between $s_i$ and its neighbors. As a result, $s_i$ is removed from $S$ and becomes a member of $\mathcal{D}$ while additional nodes of $s_i$ may be added to $\mathcal{S}$. Furthermore, nodes in $\mathcal{D}$ can be split into two groups: the core node set $\mathcal{C}$, where any node $c_i\in \mathcal{C}$ has no outward links, which means all possible neighbors of $c_i$ belong to $\mathcal{D}$; and the boundary node set $\mathcal{B}$, where any node $b_i\in \mathcal{B}$ have at least one possible neighbor in $\mathcal{S}$. Figure \ref{Local_Community_Definition} shows node sets $\mathcal{D}$, $\mathcal{S}$, $\mathcal{C}$ and $\mathcal{B}$ in a network.

\begin{figure}
\centering
\includegraphics[scale = 0.12]{\main/img/localCommunityDefinition.jpeg}
\caption{Local Community Definition}
\label{Local_Community_Definition}
\end{figure}

\subsection{Full Algorithm Description}
Similar to the algorithm mentioned in Section 3.2, our algorithm also firstly place the start node in the community. At each step, we sort candidate nodes based on their $\mathcal{K}$ (first a few steps) or $\mathcal{UR}$ values (other steps). After all candidate nodes are sorted, the algorithm will add the first node which can increase the community's $\mathcal{UR}$ to the community $\mathcal{C}$. This process will stop when there are no remaining nodes in $\mathcal{S}$ which can increase the community's $\mathcal{UR}$. It is worth noting that candidate nodes are sorted based on $\mathcal{K}$ value in the first a few steps. However, the number of steps is not yet decided. It is a tunable hyper-parameter, and we use $\lambda$ to represent it. (We will demonstrate how to find the optimal $\lambda$ in Section 5.3.) The full algorithm description can be found in Algorithm 1.
% The full algorithm includes the \textbf{Discovery Phase} and the \textbf{Examination Phase}. In the discovery phase, one can sort candidate nodes based on their $\mathcal{K}$ (first a few steps) or $\mathcal{UR}$ values at other steps. After all candidate nodes are sorted, the algorithm will add the first node which can increase the community's $\mathcal{UR}$ to the community $\mathcal{C}$. This process will stop when there are no remaining nodes in $\mathcal{S}$ which can increase the community's $\mathcal{UR}$. In the examination phase, nodes in $\mathcal{C}$ which have more neighbors outside $\mathcal{C}$ than inside will be removed except the start node. Here, $N_i^{in}$ denotes the expected number of edges between node $n_i$ and other nodes within the community, while $N_i^{out}$ denotes the expected number of edges between node $n_i$ and nodes which do not belong to the community. Besides, it is worth noting that candidate nodes are sorted based on $\mathcal{K}$ value in the first a few steps in the discovery phase. However, the number of steps is not yet decided. It is a tunable hyper-parameter, and we use $\lambda$ to represent it. (We will demonstrate how to find the optimal $\lambda$ in Section 3.3.) The full algorithm description can be found in Algorithm 1.
%using $\mathcal{K}$ at first 3 steps and $\mathcal{UR}$ at future steps to evaluate the relationship between candidate node and the existing community,

%The full algorithm description is as follows:

\begin{algorithm}
  \KwData{A network $\mathcal{G}$, a start node $n_0$ and number of steps $\lambda$.}
  \KwResult{A local community for $n_0$}% and its quality score R.
  Add $n_0$ to $\mathcal{D}$ and $\mathcal{B}$, add all $n_0$'s neighbors to $S$, $\mathcal{UR} \leftarrow 0$\;
  %// \textbf{Discovery Phase}\;
  \Repeat{no new node is added to $\mathcal{D}$}{
  	Array $nodelist \leftarrow [ ]$\;
	\For{each $n_i \in S$}{
        Compute $\mathcal{UR}_i$\; 
        // $\mathcal{UR}_i$ represents the $\mathcal{UR}$ value after adding node $n_i$\;
        Compute $\mathcal{K}_i$\;
        Add $n_i$ to $nodelist$\;
%         \If{$n_i.K>threshold$}{
%         	Add $n_i$ to $nodelist$\;
%         }
    }
        %Sort the node $n_i \in S$ first by $K_i$, second by $R_i$\;
        \uIf{$|\mathcal{D}|<\lambda$}{
        Sort $nodelist$ first by $\mathcal{K}_i$, then by $\mathcal{UR}_i$\;}
        \Else{
        Sort $nodelist$ first by $\mathcal{UR}_i$, then by $\mathcal{K}_i$\;}
        // If some nodes have same $\mathcal{K}_i$ and $\mathcal{UR}_i$, break ties randomly\;
    \For{each $n_i \in nodelist$}{
    	\If{$\mathcal{UR}_i>\mathcal{UR}$}{
        	$\mathcal{UR} \leftarrow \mathcal{UR}_i$\;
        	Add $n_i$ to $\mathcal{D}$\;
            Remove $n_i$ from $\mathcal{S}$\;
            Update $\mathcal{B}$, $\mathcal{D}$\;
            Update shell nodes possibility based on Equation (10)\;
            \textbf{break} for loop
        }
    }
  }
     \Return{$\mathcal{D}$}
  \caption{Local Community Identification with Edge Uncertainty}
   \end{algorithm}
   
\subsection{Evaluation}
The most important part is to evaluate our algorithm on real-world networks and synthetic networks. In this section, we compare our algorithm (UR+K) and other algorithms by supervised evaluation and unsupervised evaluation. The name for these algorithms and their corresponding descriptions are shown in Table \ref{Algorithm_list}. We mainly focus on the comparison between our algorithm and the other two local community detection algorithms (R and UR). Though Louvain algorithm is not a local community detection algorithm, we also compare our algorithm with it and its variant because it is also a greedy optimization method and it is always regarded as a baseline in the research of community mining. As mentioned previously, in our algorithm, $\lambda$ is a tunable hyper-parameter. In this section, we choose $\lambda=3$, and we will demonstrate how we find the best $\lambda$ in Section 5.3.

All uncertain networks are randomly generated based on deterministic network. To get more reliable results, for each deterministic network, we randomly generate 100 uncertain networks, and all values shown in result tables (figures) are average values over 100 uncertain networks.

\begin{table}[]
\centering
\caption{Algorithm List}
\label{Algorithm_list}
\begin{tabular}{c||c}
\hline
Algorithm                                                                      & Description                                                                                                                                                                \\ \hline
R                                                                     & \begin{tabular}[c]{@{}c@{}}original local community detection algorithm \\ based on the local modularity R\end{tabular}                                                    \\ \hline
UR                                                                    & \begin{tabular}[c]{@{}c@{}}original local community detection algorithm \\ based on the uncertain version local \\ modularity UR as mentioned in  Equation(2)\end{tabular} \\ \hline
UR+K                                                                  & \begin{tabular}[c]{@{}c@{}}our algorithm as mentioned in Algorithm1\end{tabular}                                            \\ \hline
Louvain                                                                        & Louvain algorithm                                                                                                                                                          \\ \hline
ULouvain                                                                       & \begin{tabular}[c]{@{}c@{}}regard probability as weight and\\  run weighted version of Louvain algorithm\end{tabular}                                                           \\ \hline
\end{tabular}
\end{table}

% \begin{table}[]
% \centering
% \caption{Algorithm List}
% \label{my-label}
% \begin{tabular}{c||c}
% \hline
% Algorithm                                                                      & Description                                                                                                                                                                \\ \hline
% original R                                                                     & \begin{tabular}[c]{@{}c@{}}original local community detection algorithm \\ based on the local modularity R\end{tabular}                                                    \\ \hline
% uncertain R                                                                    & \begin{tabular}[c]{@{}c@{}}original local community detection algorithm \\ based on the uncertain version local \\ modularity UR as mentioned in  equation(2)\end{tabular} \\ \hline
% uncertain R+K                                                                  & \begin{tabular}[c]{@{}c@{}}our algorithm as mentioned in Algorithm1,\\  however, examination phase is not included\end{tabular}                                            \\ \hline
% \begin{tabular}[c]{@{}c@{}}uncertain R+K\\ with Examination Phase\end{tabular} & our algorithm as mentioned in Algorithm1                                                                                                                                   \\ \hline
% Louvain                                                                        & Louvain algorithm                                                                                                                                                          \\ \hline
% ULouvain                                                                       & \begin{tabular}[c]{@{}c@{}}weighted version of Louvain algorithm by\\  regard probability as weight\end{tabular}                                                           \\ \hline
% \end{tabular}
% \end{table}

\subsection*{Supervised Evaluation}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
One way to compare community detection results is supervised evaluation. We use similar evaluation method as mentioned in \cite{chen2009local}. We provide networks with absolute community ground truth to the algorithm, but limit its access to network information to local nodes only (Louvain and ULouvain algorithms are allowed to use global information). The only way for the algorithm to obtain more network knowledge is to expand the community, one node at a time. Therefore, we can evaluate our algorithm based the comparison between ground truth and results obtained by our algorithm, while satisfying limitations for local community identification.

We compare our algorithm with other algorithms on 2 real-world networks and 1 synthetic network. For each network, each node is taken as the start point for algorithms one by one. Assume the start point is $n_i$, we use $\mathcal{D}_{i}$ to represent the local community for the start point $n_i$ after running local community detection algorithms, and we use $\mathcal{D}_{i^\prime}$ to represent the set of nodes which have the same label as the start node $n_i$. To quantify the accuracy of local community detection algorithms, based on the ground truth, we use f1-measure as our evaluation metrics. F1-measure is defined as the harmonic mean of precision and recall. The definition of precision, recall and f1-measure are as follows:
\begin{equation}
\text{precision} = \frac{\sum_{n_i\in \mathcal{G}}|\mathcal{D}_{i} \cap \mathcal{D}_{i\prime}|}{\sum_{n_i\in \mathcal{G}}|\mathcal{D}_{i}|}
\end{equation}
\begin{equation}
\text{recall} = \frac{\sum_{n_i\in \mathcal{G}}|\mathcal{D}_{i} \cap \mathcal{D}_{i\prime}|}{\sum_{n_i\in \mathcal{G}}|\mathcal{D}_{i\prime}|}
\end{equation}
\begin{equation}
\text{f1-measure} = \frac{2}{\frac{1}{\text{precision}}+\frac{1}{\text{recall}}}
\end{equation}
% \begin{equation}
% \text{precision} = \frac{\sum_{V_n\in G}|G_{n} \cap G_{n\prime}|}{\sum_{V_n\in G}|G_{n}|}
% \end{equation}
% \begin{equation}
% \text{recall} = \frac{\sum_{V_n\in G}|G_{n} \cap G_{n\prime}|}{\sum_{V_n\in G}|G_{n\prime}|}
% \end{equation}
% \begin{equation}
% \text{f1-measure} = \frac{2}{\frac{1}{\text{precision}}+\frac{1}{\text{recall}}}
% \end{equation}

% \begin{equation}
% Precision=\frac{\sum_{z\in G}\text{number of nodes which are in the same local community as node $z$ and have the same label as node $z$}}{\sum_{z\in G}\text{number of nodes which are in the same local community as node $z$}}
% \end{equation}

%Since the datasets we used all have ground truth community labels, we 

The results on three networks are shown in Figure 4.

% \begin{figure}
% \includegraphics[scale = 0.23]{image4.jpeg}
% \centering
% \includegraphics[scale = 0.23]{image5.jpeg}
% \centering
% \includegraphics[scale = 0.23]{image6.jpeg}
% \caption{Algorithm results on karate club, football and synthetic networks. For each network, we add 10\%, 20\%, 30\% and 40\% non-existential edges to the original network. The original network is also regarded as a special case of uncertain network. Therefore, each certain network can generate 5 uncertain networks in total. For each uncertain network, we compare our algorithm (UR+K) with the other 4 algorithms. The precision, recall and f1-measure values of each algorithm are all shown in the graphs. The f1-measure values gained by different algorithms are represented by different colored bars. Each f1-measure value's corresponding precision and recall values are represented by its inner black bar and external white bar respectively. For example, the part wrapped in a red frame represents the results gained by algorithm R on original karate club data. The f1-measure, precision and recall values are 0.6603, 0.848 and 0.5406 respectively.}
% \label{cl_enron}
% \end{figure}

% \begin{figure}[H]
% \includegraphics[scale = 0.23]{image1.jpeg}
% \centering
% \end{figure}
% \begin{figure}[H]
% \includegraphics[scale = 0.23]{image2.jpeg}
% \centering
% \end{figure}
% \begin{figure}[H]
% \includegraphics[scale = 0.23]{image3.jpeg}
% \centering
% \caption{Results}
% \label{cl_enron}
% \end{figure}


% \begin{table}[htbp]
% \begin{center}
% \noindent\makebox[\textwidth]{%
% \begin{tabularx}{1.568\textwidth}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Noise} & \multicolumn{18}{c|}{Algorithm Results For Karate Club Dataset}                                                                                                                                                                                                              \\ \cline{2-19} 
%                        & \multicolumn{3}{c|}{original R} & \multicolumn{3}{c|}{uncertain R} & \multicolumn{3}{c|}{uncertain R+K} & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}uncertain R+K with\\ Examination Phase\end{tabular}} & \multicolumn{3}{c|}{Louvain} & \multicolumn{3}{c|}{ULouvain} \\ \cline{2-19} 
%                        & R         & P        & F1       & R         & P         & F1       & R          & P         & F1        & R                               & P                               & F1                              & R        & P       & F1      & R        & P        & F1      \\ \hline
% No                     & 0.5316  & 0.8347  & 0.6495  & 0.5466  & 0.858  & 0.6678  & 0.7249  & 0.8248  & 0.7716  & 0.7024  & 0.8219  & 0.7575  & 0.4913  & 0.8712  & 0.6283  & 0.4913  & 0.8712  & 0.6283  \\ \hline
% 10\%                   & 0.5612  & 0.7935  & 0.6552  & 0.5349  & 0.816  & 0.6436  & 0.7201  & 0.8278  & 0.7672  & 0.7028  & 0.8397  & 0.7622  & 0.5135  & 0.869  & 0.6427  & 0.4991  & 0.8667  & 0.631  \\ \hline
% 20\%                   & 0.5907  & 0.6828  & 0.6262  & 0.5708  & 0.7194  & 0.6313  & 0.7723  & 0.7537  & 0.7582  & 0.7519  & 0.7796  & 0.7589  & 0.4888  & 0.8516  & 0.6177  & 0.4618  & 0.8118  & 0.5862  \\ \hline
% 30\%                   & 0.7  & 0.6433  & 0.6676  & 0.6104  & 0.7406  & 0.664  & 0.8206  & 0.7328  & 0.7694  & 0.7978  & 0.7476  & 0.7659  & 0.4979  & 0.8753  & 0.6307  & 0.4886  & 0.836  & 0.6131  \\ \hline
% 40\%                   & 0.7721  & 0.5344  & 0.6274  & 0.6115  & 0.6375  & 0.6151  & 0.7865  & 0.67  & 0.7141  & 0.7635  & 0.6809  & 0.708  & 0.4716  & 0.8115  & 0.5929  & 0.4266  & 0.8071  & 0.5553  \\ \hline

% \end{tabularx}}
% \end{center}
% \caption{Algorithm Results For Karate Club Dataset}
% \end{table}

% \begin{table}[htbp]
% \begin{center}
% \noindent\makebox[\textwidth]{%
% \begin{tabularx}{1.568\textwidth}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Noise} & \multicolumn{18}{c|}{Algorithm Results For Football Dataset}                                                                                                                                                                                                              \\ \cline{2-19} 
%                        & \multicolumn{3}{c|}{original R} & \multicolumn{3}{c|}{uncertain R} & \multicolumn{3}{c|}{uncertain R+K} & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}uncertain R+K with\\ Examination Phase\end{tabular}} & \multicolumn{3}{c|}{Louvain} & \multicolumn{3}{c|}{ULouvain} \\ \cline{2-19} 
%                        & R         & P        & F1       & R         & P         & F1       & R          & P         & F1        & R                               & P                               & F1                              & R        & P       & F1      & R        & P        & F1      \\ \hline
% No                     & 0.7512  & 0.6548  & 0.6997  & 0.7573  & 0.6599  & 0.7052  & 0.9208  & 0.9106  & 0.9156  & 0.9147  & 0.9227  & 0.9187  & 0.919  & 0.7682  & 0.8369  & 0.919  & 0.7682  & 0.8369  \\ \hline
% 10\%                   & 0.7727  & 0.6488  & 0.7051  & 0.787  & 0.6877  & 0.7334  & 0.9075  & 0.8475  & 0.876  & 0.877  & 0.8856  & 0.881  & 0.9211  & 0.6825  & 0.7817  & 0.9214  & 0.6603  & 0.766  \\ \hline
% 20\%                   & 0.7721  & 0.6401  & 0.6993  & 0.7885  & 0.6716  & 0.7248  & 0.9111  & 0.8426  & 0.8752  & 0.8599  & 0.8872  & 0.8731  & 0.9245  & 0.6703  & 0.7755  & 0.9217  & 0.7076  & 0.7983  \\ \hline
% 30\%                   & 0.7638  & 0.5681  & 0.6508  & 0.7748  & 0.6054  & 0.6787  & 0.9137  & 0.7852  & 0.8434  & 0.8047  & 0.8286  & 0.8151  & 0.9223  & 0.6237  & 0.7422  & 0.9229  & 0.6805  & 0.7792  \\ \hline
% 40\%                   & 0.7592  & 0.4413  & 0.5569  & 0.7736  & 0.5432  & 0.6355  & 0.9085  & 0.7346  & 0.8093  & 0.766  & 0.7696  & 0.7638  & 0.9236  & 0.6168  & 0.7371  & 0.9204  & 0.6841  & 0.7815  \\ \hline

% \end{tabularx}}
% \end{center}
% \caption{Algorithm Results For Football Dataset}
% \end{table}

% \begin{table}[htbp]
% \begin{center}
% \noindent\makebox[\textwidth]{%
% \begin{tabularx}{1.568\textwidth}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Noise} & \multicolumn{18}{c|}{Algorithm Results For Synthetic Network}                                                                                                                                                                                                              \\ \cline{2-19} 
%                        & \multicolumn{3}{c|}{original R} & \multicolumn{3}{c|}{uncertain R} & \multicolumn{3}{c|}{uncertain R+K} & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}uncertain R+K with\\ Examination Phase\end{tabular}} & \multicolumn{3}{c|}{Louvain} & \multicolumn{3}{c|}{ULouvain} \\ \cline{2-19} 
%                        & R         & P        & F1       & R         & P         & F1       & R          & P         & F1        & R                               & P                               & F1                              & R        & P       & F1      & R        & P        & F1      \\ \hline
% No                     & 0.6301  & 0.7057  & 0.6657  & 0.6254  & 0.7033  & 0.6621  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  & 1.0  \\ \hline
% 10\%                  & 0.6217  & 0.6686  & 0.6442  & 0.6474  & 0.6922  & 0.6688  & 0.974  & 0.9288  & 0.9501  & 0.9545  & 0.9579  & 0.9557  & 0.9969  & 0.9971  & 0.997  & 0.9817  & 0.9802  & 0.9809  \\ \hline
% 20\%                   & 0.6012  & 0.6023  & 0.6014  & 0.6526  & 0.6679  & 0.6597  & 0.9711  & 0.8762  & 0.9194  & 0.9325  & 0.9151  & 0.9217  & 0.9911  & 0.9922  & 0.9917  & 0.9823  & 0.9802  & 0.9812  \\ \hline
% 30\%                   & 0.628  & 0.5332  & 0.5728  & 0.6348  & 0.619  & 0.6257  & 0.969  & 0.855  & 0.9053  & 0.9044  & 0.8996  & 0.8982  & 0.9852  & 0.9865  & 0.9859  & 0.9706  & 0.9681  & 0.9694  \\ \hline
% 40\%                   & 0.7049  & 0.3795  & 0.4872  & 0.643  & 0.4804  & 0.547  & 0.967  & 0.6737  & 0.7854  & 0.8871  & 0.7072  & 0.7751  & 0.9773  & 0.9711  & 0.9736  & 0.952  & 0.9541  & 0.953  \\ \hline

% \end{tabularx}}
% \end{center}
% \caption{Algorithm Results For Synthetic Network}
% \end{table}

From Figure 4, we can observe that our algorithm (UR+K) can significantly outperform other algorithms in terms of f1-measure in karate club dataset and football dataset. Though our algorithm can not beat Louvain and ULouvain algorithms in synthetic network, it also shows competitive detection accuracy, and it still performs better than algorithms R and UR on synthetic dataset. Considering our algorithm only use local information, while Louvain and ULouvain use global information, these results are still satisfying. %(NEED POLISHED)

% When we compare results achieved by our algorithm (uncertain R+K with Examination Phase) and uncertain R+K, we find that they gain almost the same f1-measure value. The former one has higher recall value, while its precision value is lower. We can also find that our algorithm tends to perform better when networks have more noises, while it is slightly worse than uncertain R+K when networks are more certain. (NEED POLISHED)

% However, there exists a problem. we have an assumption, we assume that the community structure does not change when we add noises.

% \begin{table}[]
% \centering
% \caption{My caption}
% \label{my-label}
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{3}{*}{Noise} & \multicolumn{24}{c|}{Algorithm Results For Karate Club Dataset}                                                                                                                                                                                                                                                                                          \\ \cline{2-25} 
%                        & \multicolumn{3}{c|}{original R} & \multicolumn{3}{c|}{uncertain R} & \multicolumn{3}{c|}{uncertain R+K} & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}uncertain R+K with\\ Examination Phase\end{tabular}} & \multicolumn{3}{c|}{Louvain} & \multicolumn{3}{c|}{ULouvain} & \multicolumn{3}{c|}{uncertain R+K/} & \multicolumn{3}{c|}{uncertain R+K-} \\ \cline{2-25} 
%                        & P         & R        & F1       & P         & R         & F1       & P          & R         & F1        & P                               & R                               & F1                              & P        & R       & F1      & P        & R        & F1      & P          & R          & F1        & P          & R          & F1        \\ \hline
% No                     & 0.5395    & 0.8465   & 0.659    & 0.5396    & 0.8453    & 0.6587   & 0.7232     & 0.8023    & 0.7607    & 0.7145                          & 0.8082                          & 0.7585                          & 0.4913   & 0.8712  & 0.6283  & 0.4913   & 0.8712   & 0.6283  & 0.5779     & 0.8329     & 0.6823    & 0.6298     & 0.833      & 0.7172    \\ \hline
% 10\%                   & 0.5566    & 0.7868   & 0.6486   & 0.5573    & 0.8089    & 0.657    & 0.7412     & 0.7561    & 0.7463    & 0.7231                          & 0.7681                          & 0.7425                          & 0.5028   & 0.8734  & 0.6366  & 0.4937   & 0.8634   & 0.6259  & 0.6489     & 0.7291     & 0.6806    & 0.6391     & 0.7457     & 0.6829    \\ \hline
% 20\%                   & 0.6287    & 0.7068   & 0.659    & 0.5689    & 0.7523    & 0.643    & 0.7595     & 0.7284    & 0.7405    & 0.7395                          & 0.7375                          & 0.7351                          & 0.4894   & 0.8522  & 0.6193  & 0.4731   & 0.8413   & 0.6031  & 0.6909     & 0.662      & 0.6694    & 0.6758     & 0.6763     & 0.6691    \\ \hline
% 35\%                   & 0.7311    & 0.5834   & 0.641    & 0.6077    & 0.6622    & 0.6254   & 0.7885     & 0.6592    & 0.7142    & 0.7698                          & 0.6638                          & 0.7085                          & 0.4742   & 0.8247  & 0.5995  & 0.4447   & 0.8174   & 0.5738  & 0.7407     & 0.5676     & 0.6367    & 0.7292     & 0.582      & 0.6407    \\ \hline
% 50\%                   & 0.8151    & 0.5309   & 0.6388   & 0.6571    & 0.604     & 0.6212   & 0.8292     & 0.6191    & 0.7062    & 0.8137                          & 0.6237                          & 0.703                           & 0.4516   & 0.7855  & 0.5702  & 0.4257   & 0.7925   & 0.5507  & 0.7857     & 0.5419     & 0.6371    & 0.7763     & 0.5503     & 0.639     \\ \hline
% \end{tabular}
% \end{table}

\subsection*{Unsupervised Evaluation}
Since we generate uncertain networks based on certain networks. When we evaluate our algorithm in the supervised way, we assume that uncertain networks have the same community ground truth as their corresponding certain networks. This assumption is true if we only add a few noise edges to uncertain networks because small number of noise edges will not have impact on the community structure. However, with the increase in the number of noise edges, some communities may merge into one community. In this scenario, using original community labels will cause problems.

To solve this problem, we also propose an unsupervised way to evaluate our algorithm. As mentioned in Section 3.1, the local community modularity $\mathcal{R}$ can be used to measure the quality of the local community. In the uncertain networks scenario, we can use $\mathcal{UR}$ to take the place of $\mathcal{R}$. Therefore, to compare different algorithms, we can compare local community modularity $\mathcal{UR}$ values after running different algorithms.

In this part, we also compare our algorithm (UR+K) with other algorithms on 2 real-networks and 1 synthetic network. The results are shown in Table \ref{unsupervised_karate}, \ref{unsupervised_football}, \ref{unsupervised_synthetic}.

% Another way to evaluate our algorithm is the unsupervised way. We can compare modularity values after running different algorithms. In this part, we use two different local modularities: (1) the first is $\mathcal{UR}$, which have already been defined in equation(2) and have been used to merge new nodes to existing community; and (2) a new modularity, $sample\mathcal{R}$, to evaluate different algorithms. $sample\mathcal{R}$ is a sample-based modularity, which  can be represented as:
% \begin{equation}
% sample\mathcal{R} = \frac{1}{N}\sum_{i=1}^{N}\mathcal{R}_i
% \end{equation}
% Given uncertain network $\mathcal{G = (V,E,P)}$, we can sample each edge in $\mathcal{G}$ according to the probability $p(e)$ to generate the possible graph $G = (V_G,E_G)$. We have $E_G \in \mathcal{E}$ and $V_G \in \mathcal{E}$. The probability $Pr|G|$ of sampling the possible graph is as follows:
% \begin{equation}
% Pr|G| = \prod_{e\in E_G}p(e)\prod_{e\in \mathcal{E}, e\notin E_G}(1-p(e))
% \end{equation}
% There are a total of $2^{|\mathcal{E}|}$ possible graphs, since each edge provides us with a binary sampling decision. The number of possible graphs become very large with the increase of number of edges. After running each algorithm, we have the local community for each start node. In evaluation, we can sample enough (e.g. 1000) possible graphs and calculate the modularity $\mathcal{R}$ based on equation (1) for each possible graph. Then we can get the $sample\mathcal{R}$ for each algorithm according to equation (13).
%According to Chernoff-Hoeffding Bound

\begin{table}[]
\centering
\caption{$\mathcal{UR}$ on Karate Club Data}
\label{unsupervised_karate}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Noise & R      & UR     & UR+K            & Louvain & ULouvain \\ \hline
No    & 0.5787 & 0.5789 & \textbf{0.654}  & 0.5604  & 0.5604   \\ \hline
10\%  & 0.584  & 0.5902 & \textbf{0.7026} & 0.5176  & 0.5258   \\ \hline
20\%  & 0.6235 & 0.5942 & \textbf{0.7462} & 0.4737  & 0.4858   \\ \hline
30\%  & 0.7065 & 0.6232 & \textbf{0.7858} & 0.4326  & 0.4583   \\ \hline
40\%  & 0.7628 & 0.6641 & \textbf{0.8601} & 0.3965  & 0.4281   \\ \hline
\end{tabular}
\end{table}

% \begin{table}[]
% \centering
% \caption{$\mathcal{UR}$ on Karate Club Data}
% \label{my-label}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% Noise & original R & uncertain R & uncertain R+K & \begin{tabular}[c]{@{}c@{}}uncertain R+K with \\ Examination Phase\end{tabular} & Louvain & ULouvain \\ \hline
% No    &  0.5783&  0.5788&  0.654&  0.6507&  0.5604&  0.5604  \\ \hline
% 10\%  &  0.5641&  0.5904&  0.6726&  0.6566&  0.5364&  0.5467  \\ \hline
% 20\%  &  0.5869&  0.5759&  0.6894&  0.6708&  0.4799&  0.5125  \\ \hline
% 30\%  &  0.7243&  0.6193&  0.7487&  0.7358&  0.4631&  0.4589  \\ \hline
% 40\%  &  0.7615&  0.6205&  0.7644&  0.7492&  0.4015&  0.4508  \\ \hline
% \end{tabular}
% \end{table}

% \begin{table}[]
% \centering
% \caption{$Sample\mathcal{R}$ on Karate Club Data}
% \label{my-label}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% Noise & original R & uncertain R & uncertain R+K & \begin{tabular}[c]{@{}c@{}}uncertain R+K with \\ Examination Phase\end{tabular} & Louvain & ULouvain \\ \hline
% No    &  0.5783&  0.5788&  0.654&  0.6507&  0.5604&  0.5604  \\ \hline
% 10\%  &  0.522&  0.5454&  0.6158&  0.6089&  0.5068&  0.5213  \\ \hline
% 20\%  &  0.5505&  0.5372&  0.6361&  0.6244&  0.4583&  0.4909  \\ \hline
% 30\%  &  0.701&  0.5822&  0.7019&  0.696&  0.4442&  0.4388  \\ \hline
% 40\%  &  0.741&  0.587&  0.7231&  0.711&  0.3878&  0.4376  \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[]
\centering
\caption{$\mathcal{UR}$ on Football Data}
\label{unsupervised_football}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Noise & R      & UR     & UR+K   & Louvain         & ULouvain        \\ \hline
No    & 0.5065 & 0.5057 & 0.5327 & \textbf{0.5497} & \textbf{0.5497} \\ \hline
10\%  & 0.4714 & 0.4826 & 0.4902 & 0.5152          & \textbf{0.5208} \\ \hline
20\%  & 0.4378 & 0.4491 & 0.454  & 0.4783          & \textbf{0.4802} \\ \hline
30\%  & 0.415  & 0.4189 & 0.4221 & 0.4384          & \textbf{0.4463} \\ \hline
40\%  & 0.4121 & 0.4008 & 0.4017 & 0.4134          & \textbf{0.4164} \\ \hline
\end{tabular}
\end{table}

% \begin{table}[]
% \centering
% \caption{$\mathcal{UR}$ on Football Data}
% \label{my-label}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% Noise & original R & uncertain R & uncertain R+K & \begin{tabular}[c]{@{}c@{}}uncertain R+K with \\ Examination Phase\end{tabular} & Louvain & ULouvain \\ \hline
% No    &  0.5066&  0.5054&  0.5327&  0.5254&  0.5497&  0.5497  \\ \hline
% 10\%  &  0.4693&  0.4868&  0.4938&  0.4762&  0.518&  0.5217  \\ \hline
% 20\%  &  0.4363&  0.4495&  0.455&  0.4219&  0.4804&  0.4792  \\ \hline
% 30\%  &  0.4172&  0.4189&  0.4203&  0.37&  0.4391&  0.4358  \\ \hline
% 40\%  &  0.4138&  0.4011&  0.3994&  0.3343&  0.4145&  0.4243  \\ \hline
% \end{tabular}
% \end{table}

% \begin{table}[]
% \centering
% \caption{$Sample\mathcal{R}$ on Football Data}
% \label{my-label}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% Noise & original R & uncertain R & uncertain R+K & \begin{tabular}[c]{@{}c@{}}uncertain R+K with \\ Examination Phase\end{tabular} & Louvain & ULouvain \\ \hline
% No    &  0.5066&  0.5054&  0.5327&  0.5254&  0.5497&  0.5497  \\ \hline
% 10\%  &  0.4682&  0.4855&  0.493&  0.4757&  0.5156&  0.5192  \\ \hline
% 20\%  &  0.4359&  0.449&  0.4548&  0.4219&  0.4791&  0.4786  \\ \hline
% 30\%  &  0.4163&  0.4186&  0.4201&  0.3699&  0.4384&  0.4354  \\ \hline
% 40\%  &  0.4123&  0.401&  0.3994&  0.3343&  0.4147&  0.4242  \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[]
\centering
\caption{$\mathcal{UR}$ on Synthetic Data}
\label{unsupervised_synthetic}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Noise & R      & UR     & UR+K            & Louvain         & ULouvain        \\ \hline
No    & 0.6018 & 0.6019 & \textbf{0.6537} & \textbf{0.6537} & \textbf{0.6537} \\ \hline
10\%  & 0.5476 & 0.5361 & \textbf{0.5928} & 0.5924          & 0.5924          \\ \hline
20\%  & 0.5016 & 0.4941 & \textbf{0.5484} & 0.5415          & 0.5414          \\ \hline
30\%  & 0.4964 & 0.4685 & \textbf{0.5306} & 0.5003          & 0.5             \\ \hline
40\%  & 0.5232 & 0.4735 & \textbf{0.5742} & 0.4635          & 0.4642          \\ \hline
\end{tabular}
\end{table}

% \begin{table}[]
% \centering
% \caption{$\mathcal{UR}$ on Synthetic Data}
% \label{my-label}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% Noise & original R & uncertain R & uncertain R+K & \begin{tabular}[c]{@{}c@{}}uncertain R+K with \\ Examination Phase\end{tabular} & Louvain & ULouvain \\ \hline
% No    &  0.6017&  0.602&  0.6537&  0.6537&  0.6537&  0.6537  \\ \hline
% 10\%  &  0.5553&  0.5571&  0.6014&  0.596&  0.5995&  0.6  \\ \hline
% 20\%  &  0.5041&  0.4957&  0.5484&  0.5346&  0.5399&  0.5418  \\ \hline
% 30\%  &  0.4759&  0.4729&  0.5285&  0.5081&  0.5038&  0.505  \\ \hline
% 40\%  &  0.56&  0.4618&  0.5008&  0.4721&  0.4683&  0.4712  \\ \hline
% \end{tabular}
% \end{table}

% \begin{table}[]
% \centering
% \caption{$Sample\mathcal{R}$ on Synthetic Data}
% \label{my-label}
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% Noise & original R & uncertain R & uncertain R+K & \begin{tabular}[c]{@{}c@{}}uncertain R+K with \\ Examination Phase\end{tabular} & Louvain & ULouvain \\ \hline
% No    &  0.6017&  0.602&  0.6537&  0.6537&  0.6537&  0.6537  \\ \hline
% 10\%  &  0.5499&  0.5519&  0.5977&  0.5927&  0.596&  0.5965  \\ \hline
% 20\%  &  0.5015&  0.4934&  0.5471&  0.5337&  0.5388&  0.5407  \\ \hline
% 30\%  &  0.4741&  0.4709&  0.5267&  0.5067&  0.5035&  0.5046  \\ \hline
% 40\%  &  0.5588&  0.4607&  0.5002&  0.4717&  0.4683&  0.4713  \\ \hline
% \end{tabular}
% \end{table}

From Table \ref{unsupervised_karate}, \ref{unsupervised_football}, \ref{unsupervised_synthetic}, we can find our algorithm (UR+K) performs the best on karate club and synthetic datasets, and the $\mathcal{UR}$ values achieved on football dataset by our algorithm are very close to the best results achieved by the  other algorithms. It is worth noting that, in the expansion steps, though UR algorithm always choose the node which gives the largest increase of $\mathcal{UR}$, while $\mathcal{UR}$ value is not mainly considered in the first a few steps in our algorithm, our algorithm finally achieved higher $\mathcal{UR}$ values than UR algorithm on all datasets.

\end{document}